{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Da76rrkm2irSgpVHcWPj50KP7D6S7wfc",
      "authorship_tag": "ABX9TyOOc3QjCxKG5IIzfQTttneP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdokamel2001/ASL-Translation-Project/blob/main/2023-10-Sprint2-Shared-Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook Contains the Shared Functions across both Notebooks"
      ],
      "metadata": {
        "id": "h513J3nqZYcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yrft04dZv8G",
        "outputId": "8ac1c097-5707-431a-d782-48408af8cc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dBt_ON5FZxG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MediaPipe Functions"
      ],
      "metadata": {
        "id": "hiWXDvl5X-zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hands = mp.solutions.hands.Hands()\n",
        "pose = mp.solutions.pose.Pose()\n",
        "face_mesh = mp.solutions.face_mesh.FaceMesh()"
      ],
      "metadata": {
        "id": "vSUKWV68ZXCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frame_landmarks(frame):\n",
        "    \"\"\"\n",
        "    Extracts landmarks from a single video frame using MediaPipe.\n",
        "\n",
        "    Args:\n",
        "        frame: A single rgb frame/image.\n",
        "\n",
        "    Returns:\n",
        "        np.array: A NumPy array containing extracted landmarks.\n",
        "        The output dimensions are (n, 3) array, where n is the number of landmarks.\n",
        "        Each row in the array represents a landmark, and each landmark is represented\n",
        "        as [x, y, z], where x, y, and z are the normalized coordinates of the landmark.\n",
        "    \"\"\"\n",
        "\n",
        "    results_hands = hands.process(frame)\n",
        "    results_pose = pose.process(frame)\n",
        "    results_face = face_mesh.process(frame)\n",
        "\n",
        "    landmarks_per_hand = 21\n",
        "    landmarks_body_pose = 33\n",
        "    landmarks_face = 468         # Max 468\n",
        "\n",
        "    all_landmarks = np.zeros((landmarks_per_hand * 2 + landmarks_body_pose + landmarks_face, 3))\n",
        "\n",
        "    if results_hands.multi_hand_landmarks:\n",
        "        all_landmarks[:landmarks_per_hand, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[0].landmark])\n",
        "        if len(results_hands.multi_hand_landmarks) > 1:\n",
        "            all_landmarks[landmarks_per_hand:landmarks_per_hand * 2, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[1].landmark])\n",
        "\n",
        "    if results_pose.pose_landmarks:\n",
        "        all_landmarks[landmarks_per_hand * 2:landmarks_per_hand * 2 + landmarks_body_pose, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])\n",
        "\n",
        "    if results_face.multi_face_landmarks:\n",
        "        # all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark[::468 // landmarks_face]])\n",
        "        all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])\n",
        "\n",
        "    return all_landmarks"
      ],
      "metadata": {
        "id": "q9sgjTLxZi3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_landmarks(video_path, start_frame=0, end_frame=-1, num_landmarks=543):\n",
        "    \"\"\"\n",
        "    Extracts landmarks from a video by processing each frame in the video.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The file path to the video to process.\n",
        "        start_frame (int): The index of the starting frame (default is 0).\n",
        "        end_frame (int): The index of the ending frame (default is -1, meaning the last frame).\n",
        "\n",
        "    Returns:\n",
        "        np.array: A NumPy array where each row corresponds to the landmarks\n",
        "        extracted from a single frame of the video within the specified frame range.\n",
        "        The dimensions of the output array are (m, n, 3), where m is the number of frames\n",
        "        within the specified range and n is the number of landmarks.\n",
        "        Each element in the array is a 3D coordinate representing a landmark's position.\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if end_frame < 0:\n",
        "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if start_frame < 0:\n",
        "        start_frame = 0\n",
        "\n",
        "    all_frame_landmarks = np.zeros((end_frame - start_frame, num_landmarks, 3))\n",
        "    frame_index = 0\n",
        "\n",
        "    while cap.isOpened() and frame_index != end_frame:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_index >= start_frame:\n",
        "            frame.flags.writeable = False\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_landmarks = get_frame_landmarks(frame)\n",
        "            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return all_frame_landmarks"
      ],
      "metadata": {
        "id": "oYA3_3nbC9x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV Functions"
      ],
      "metadata": {
        "id": "FuxgJKAHcg6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(input_path, output_path, npy_file, start_frame=0, end_frame=-1):\n",
        "    \"\"\"\n",
        "    Reads a video from the input file, overlays landmarks on each frame, and saves the result to an output video file.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input video file.\n",
        "        output_path (str): The path to save the output video with overlaid facial landmarks.\n",
        "        npy_file (str): The path to a NumPy file containing facial landmarks data for each frame.\n",
        "        start_frame (int): The index of the starting frame for landmark overlay (default is 0).\n",
        "        end_frame (int): The index of the ending frame for landmark overlay (default is -1, meaning the last frame).\n",
        "\n",
        "    Description:\n",
        "        This function reads a video from the input file, extracts facial landmarks data from a NumPy file,\n",
        "        and overlays landmarks on each frame of the video. The frames within the specified range,\n",
        "        from 'start_frame' (inclusive) to 'end_frame' (exclusive), are processed. Facial landmarks are drawn as\n",
        "        red circles on the face, hands, and body in each frame. The output video is saved to the 'output_path'\n",
        "        with the same resolution and frame rate as the input video.\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    landmarks_data = np.load(npy_file)\n",
        "    frame_index = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_index >= start_frame and frame_index < end_frame:\n",
        "            landmarks_frame = landmarks_data[frame_index - start_frame]\n",
        "            landmarks = [(int(x * width), int(y * height)) for x, y, z in landmarks_frame]\n",
        "            for x, y in landmarks:\n",
        "                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()"
      ],
      "metadata": {
        "id": "BR-7j4H0cjkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV Test"
      ],
      "metadata": {
        "id": "XIgS3n-MlQTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Tasks/2023-10-Sprint2/OpenCV_Test\" \"/content\""
      ],
      "metadata": {
        "id": "2l4Q7Cl7lSxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw('/content/OpenCV_Test/mCjHYreiZ24.mp4', '/content/Edited.mp4', '/content/OpenCV_Test/thanks.npy', 0, 89)"
      ],
      "metadata": {
        "id": "Hwye28FSlpnh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}