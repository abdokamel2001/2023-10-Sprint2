{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abd0kamel/2023-10-sprint2-kaggle?scriptVersionId=148289293\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# WLASL Dataset Sign Language Model","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# from IPython.core.display import clear_output\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T03:48:09.863398Z","iopub.execute_input":"2023-10-28T03:48:09.863651Z","iopub.status.idle":"2023-10-28T03:48:09.867822Z","shell.execute_reply.started":"2023-10-28T03:48:09.863627Z","shell.execute_reply":"2023-10-28T03:48:09.867077Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the JSON file\nwith open('/kaggle/input/wlasl-processed/WLASL_v0.3.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Words to filter by\nwords_to_filter = [\"where\",\"hello\",\"thank you\", \"go\",\"drop\",\"here\",\"traffic\",\"good\",\"bad\",\"today\"]\n\n# Filter the data\nfiltered_data = [item for item in data if item.get(\"gloss\") in words_to_filter]\n\n# Extract the first 5 \"bbox\" elements from each item\nfor item in filtered_data:\n    item[\"instances\"] = item[\"instances\"][:5]\n\n# Save the filtered data with the first 5 \"bbox\" elements to a JSON file\nwith open('filtered_data_with_5_bbox.json', 'w') as output_file:\n    json.dump(filtered_data, output_file, indent=4)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:48:09.886616Z","iopub.execute_input":"2023-10-28T03:48:09.887393Z","iopub.status.idle":"2023-10-28T03:48:10.096766Z","shell.execute_reply.started":"2023-10-28T03:48:09.887368Z","shell.execute_reply":"2023-10-28T03:48:10.096103Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n\ninput_folder = '/kaggle/input/wlasl-processed/videos'","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:48:10.098187Z","iopub.execute_input":"2023-10-28T03:48:10.098605Z","iopub.status.idle":"2023-10-28T03:48:10.102321Z","shell.execute_reply.started":"2023-10-28T03:48:10.098579Z","shell.execute_reply":"2023-10-28T03:48:10.101487Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"video_files = [f for f in os.listdir(input_folder) if f.endswith('.mp4')]\nprint(len(video_files)) ","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:48:10.103463Z","iopub.execute_input":"2023-10-28T03:48:10.104201Z","iopub.status.idle":"2023-10-28T03:48:11.171522Z","shell.execute_reply.started":"2023-10-28T03:48:10.10418Z","shell.execute_reply":"2023-10-28T03:48:11.170725Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"11980\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/working/filtered_data_with_5_bbox.json', 'r') as f:\n    data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:48:22.835157Z","iopub.execute_input":"2023-10-28T03:48:22.835428Z","iopub.status.idle":"2023-10-28T03:48:22.848965Z","shell.execute_reply.started":"2023-10-28T03:48:22.835408Z","shell.execute_reply":"2023-10-28T03:48:22.848098Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"gloss_list = [] # done\nvideo_name_list = [] # done\nstart_frame = [] # done\nend_frame = [] # done\nvid_dir = '/kaggle/input/wlasl-processed/videos'\nprogress = 0\n\nfor word in data:\n    for i in range(5):\n        vid_name = os.path.join(vid_dir, f'{word[\"instances\"][i][\"video_id\"]}.mp4')\n        start = word['instances'][i]['frame_start']\n        end = word['instances'][i]['frame_end']\n        label = word['gloss']\n\n        video_name_list.append(vid_name)\n        gloss_list.append(label)\n        start_frame.append(start)\n        end_frame.append(end)\n\n        progress += 1\n        print(f'Progress: {progress} / {len(data)*5}')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:05:49.232836Z","iopub.execute_input":"2023-10-28T04:05:49.233168Z","iopub.status.idle":"2023-10-28T04:05:49.240511Z","shell.execute_reply.started":"2023-10-28T04:05:49.233147Z","shell.execute_reply":"2023-10-28T04:05:49.239555Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Progress: 1 / 50\nProgress: 2 / 50\nProgress: 3 / 50\nProgress: 4 / 50\nProgress: 5 / 50\nProgress: 6 / 50\nProgress: 7 / 50\nProgress: 8 / 50\nProgress: 9 / 50\nProgress: 10 / 50\nProgress: 11 / 50\nProgress: 12 / 50\nProgress: 13 / 50\nProgress: 14 / 50\nProgress: 15 / 50\nProgress: 16 / 50\nProgress: 17 / 50\nProgress: 18 / 50\nProgress: 19 / 50\nProgress: 20 / 50\nProgress: 21 / 50\nProgress: 22 / 50\nProgress: 23 / 50\nProgress: 24 / 50\nProgress: 25 / 50\nProgress: 26 / 50\nProgress: 27 / 50\nProgress: 28 / 50\nProgress: 29 / 50\nProgress: 30 / 50\nProgress: 31 / 50\nProgress: 32 / 50\nProgress: 33 / 50\nProgress: 34 / 50\nProgress: 35 / 50\nProgress: 36 / 50\nProgress: 37 / 50\nProgress: 38 / 50\nProgress: 39 / 50\nProgress: 40 / 50\nProgress: 41 / 50\nProgress: 42 / 50\nProgress: 43 / 50\nProgress: 44 / 50\nProgress: 45 / 50\nProgress: 46 / 50\nProgress: 47 / 50\nProgress: 48 / 50\nProgress: 49 / 50\nProgress: 50 / 50\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'number of words = {len(gloss_list)}\\nnumber of videos = {len(video_name_list)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:11.157307Z","iopub.execute_input":"2023-10-28T04:09:11.157622Z","iopub.status.idle":"2023-10-28T04:09:11.161792Z","shell.execute_reply.started":"2023-10-28T04:09:11.157598Z","shell.execute_reply":"2023-10-28T04:09:11.161224Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"number of words = 50\nnumber of videos = 50\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Specify the file path to your .npy file\nfile_path = \"/kaggle/input/opencvtest/thanks.npy\"\n\n# Load the .npy file\nloaded_data = np.load(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:11.406305Z","iopub.execute_input":"2023-10-28T04:09:11.406619Z","iopub.status.idle":"2023-10-28T04:09:11.413001Z","shell.execute_reply.started":"2023-10-28T04:09:11.406597Z","shell.execute_reply":"2023-10-28T04:09:11.412219Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"loaded_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:11.613903Z","iopub.execute_input":"2023-10-28T04:09:11.614172Z","iopub.status.idle":"2023-10-28T04:09:11.618791Z","shell.execute_reply.started":"2023-10-28T04:09:11.61415Z","shell.execute_reply":"2023-10-28T04:09:11.618003Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"(90, 543, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef draw_landmarks(input_path, output_path, npy_file, start_frame=0, end_frame=-1):\n    \"\"\"\n    Reads a video from the input file, overlays landmarks on each frame, and saves the result to an output video file.\n\n    Args:\n        input_path (str): The path to the input video file.\n        output_path (str): The path to save the output video with overlaid facial landmarks.\n        npy_file (str): The path to a NumPy file containing facial landmarks data for each frame.\n        start_frame (int): The index of the starting frame for landmark overlay (default is 0).\n        end_frame (int): The index of the ending frame for landmark overlay (default is -1, meaning the last frame).\n\n    Description:\n        This function reads a video from the input file, extracts facial landmarks data from a NumPy file,\n        and overlays landmarks on each frame of the video. The frames within the specified range,\n        from 'start_frame' (inclusive) to 'end_frame' (exclusive), are processed. Facial landmarks are drawn as\n        red circles on the face, hands, and body in each frame. The output video is saved to the 'output_path'\n        with the same resolution and frame rate as the input video.\n    \"\"\"\n\n    cap = cv2.VideoCapture(input_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    landmarks_data = np.load(npy_file)\n    frame_index = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_index >= start_frame and frame_index < end_frame:\n            landmarks_frame = landmarks_data[frame_index - start_frame]\n            landmarks = [(int(x * width), int(y * height)) for x, y, z in landmarks_frame]\n            for x, y in landmarks:\n                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n            out.write(frame)\n        else:\n            # out.write(frame) # Enable if you want the full video\n            pass\n        frame_index += 1\n\n    cap.release()\n    out.release()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:11.862921Z","iopub.execute_input":"2023-10-28T04:09:11.863172Z","iopub.status.idle":"2023-10-28T04:09:11.871553Z","shell.execute_reply.started":"2023-10-28T04:09:11.863151Z","shell.execute_reply":"2023-10-28T04:09:11.870521Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"! pip -q install mediapipe\nimport mediapipe as mp","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:59:56.931803Z","iopub.execute_input":"2023-10-28T03:59:56.932319Z","iopub.status.idle":"2023-10-28T04:00:05.487846Z","shell.execute_reply.started":"2023-10-28T03:59:56.932293Z","shell.execute_reply":"2023-10-28T04:00:05.486781Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"hands = mp.solutions.hands.Hands()\npose = mp.solutions.pose.Pose()\nface_mesh = mp.solutions.face_mesh.FaceMesh()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:15.971702Z","iopub.execute_input":"2023-10-28T04:09:15.972268Z","iopub.status.idle":"2023-10-28T04:09:16.005674Z","shell.execute_reply.started":"2023-10-28T04:09:15.972223Z","shell.execute_reply":"2023-10-28T04:09:16.004883Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"def get_frame_landmarks(frame):\n    \"\"\"\n    Extracts landmarks from a single video frame using MediaPipe.\n\n    Args:\n        frame: A single rgb frame/image.\n\n    Returns:\n        np.array: A NumPy array containing extracted landmarks.\n        The output dimensions are (n, 3) array, where n is the number of landmarks.\n        Each row in the array represents a landmark, and each landmark is represented\n        as [x, y, z], where x, y, and z are the normalized coordinates of the landmark.\n    \"\"\"\n\n    results_hands = hands.process(frame)\n    results_pose = pose.process(frame)\n    results_face = face_mesh.process(frame)\n\n    landmarks_per_hand = 21\n    landmarks_body_pose = 33\n    landmarks_face = 468         # Max 468\n\n    all_landmarks = np.empty((landmarks_per_hand * 2 + landmarks_body_pose + landmarks_face, 3))\n\n    if results_hands.multi_hand_landmarks:\n        all_landmarks[:landmarks_per_hand, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[0].landmark])\n        if len(results_hands.multi_hand_landmarks) > 1:\n            all_landmarks[landmarks_per_hand:landmarks_per_hand * 2, :] = np.array(\n                [(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[1].landmark])\n\n    if results_pose.pose_landmarks:\n        all_landmarks[landmarks_per_hand * 2:landmarks_per_hand * 2 + landmarks_body_pose, :] = np.array(\n            [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])\n\n    if results_face.multi_face_landmarks:\n        # all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array(\n            # [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark[::468 // landmarks_face]]\n            # )\n\n        all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array(\n            [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark]\n            )\n\n    return all_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:16.335484Z","iopub.execute_input":"2023-10-28T04:09:16.335769Z","iopub.status.idle":"2023-10-28T04:09:16.343546Z","shell.execute_reply.started":"2023-10-28T04:09:16.335745Z","shell.execute_reply":"2023-10-28T04:09:16.342624Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def get_video_landmarks(video_path, start_frame=0, end_frame=-1, num_landmarks=543):\n    \"\"\"\n    Extracts landmarks from a video by processing each frame in the video.\n\n    Args:\n        video_path (str): The file path to the video to process.\n        start_frame (int): The index of the starting frame (default is 0).\n        end_frame (int): The index of the ending frame (default is -1, meaning the last frame).\n\n    Returns:\n        np.array: A NumPy array where each row corresponds to the landmarks\n        extracted from a single frame of the video within the specified frame range.\n        The dimensions of the output array are (m, n, 3), where m is the number of frames\n        within the specified range and n is the number of landmarks.\n        Each element in the array is a 3D coordinate representing a landmark's position.\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    if end_frame < 0:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if start_frame < 0:\n        start_frame = 0\n    if end_frame < start_frame:\n        start_frame, end_frame = end_frame, start_frame\n        \n\n    all_frame_landmarks = np.empty((end_frame - start_frame, num_landmarks, 3))\n    frame_index = 0\n\n    while cap.isOpened() and frame_index != end_frame:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_index >= start_frame:\n            frame.flags.writeable = False\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_landmarks = get_frame_landmarks(frame)\n            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n\n        frame_index += 1\n\n    cap.release()\n\n    return all_frame_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:19.786842Z","iopub.execute_input":"2023-10-28T04:09:19.78717Z","iopub.status.idle":"2023-10-28T04:09:19.7938Z","shell.execute_reply.started":"2023-10-28T04:09:19.787148Z","shell.execute_reply":"2023-10-28T04:09:19.793244Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"lm = get_video_landmarks(video_name_list[0], start_frame=start_frame[0], end_frame=end_frame[0])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:20.713353Z","iopub.execute_input":"2023-10-28T04:09:20.714023Z","iopub.status.idle":"2023-10-28T04:09:20.721306Z","shell.execute_reply.started":"2023-10-28T04:09:20.713992Z","shell.execute_reply":"2023-10-28T04:09:20.720476Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/test.npy',lm)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:22.525622Z","iopub.execute_input":"2023-10-28T04:09:22.525946Z","iopub.status.idle":"2023-10-28T04:09:22.530686Z","shell.execute_reply.started":"2023-10-28T04:09:22.525924Z","shell.execute_reply":"2023-10-28T04:09:22.52963Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"np.sum(lm!=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:24.496728Z","iopub.execute_input":"2023-10-28T04:09:24.497021Z","iopub.status.idle":"2023-10-28T04:09:24.501947Z","shell.execute_reply.started":"2023-10-28T04:09:24.497001Z","shell.execute_reply":"2023-10-28T04:09:24.501408Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"1704"},"metadata":{}}]},{"cell_type":"code","source":"lm[0]","metadata":{"execution":{"iopub.status.busy":"2023-10-28T04:09:45.647773Z","iopub.execute_input":"2023-10-28T04:09:45.648113Z","iopub.status.idle":"2023-10-28T04:09:45.655261Z","shell.execute_reply.started":"2023-10-28T04:09:45.648088Z","shell.execute_reply":"2023-10-28T04:09:45.654324Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"array([[4.04738577e-320, 3.55727265e-322, 0.00000000e+000],\n       [4.87994243e-310, 4.94065646e-324, 4.87994243e-310],\n       [4.87994214e-310, 4.94065646e-324, 4.87994243e-310],\n       ...,\n       [5.58294180e-321, 0.00000000e+000, 1.36362118e-321],\n       [4.87994243e-310, 5.58788245e-321, 0.00000000e+000],\n       [1.36362118e-321, 4.87994243e-310, 5.59282311e-321]])"},"metadata":{}}]},{"cell_type":"code","source":"import os\ninput_video = '/kaggle/input/opencvtest/mCjHYreiZ24.mp4'\noutput_video = '/kaggle/working/ahhhhhhhh.mp4'\nnpy_path = \"/kaggle/working/numpyOut/0.npy\"\n\ndraw('/kaggle/input/wlasl-processed/videos/00335.mp4', output_video, '/kaggle/input/opencvtest/thanks.npy',start_frame=start_frame[0],end_frame=end_frame[0])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:45:40.852694Z","iopub.execute_input":"2023-10-28T03:45:40.853109Z","iopub.status.idle":"2023-10-28T03:45:40.892596Z","shell.execute_reply.started":"2023-10-28T03:45:40.853079Z","shell.execute_reply":"2023-10-28T03:45:40.891561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_name_list[0]","metadata":{"execution":{"iopub.status.busy":"2023-10-28T03:44:17.844323Z","iopub.execute_input":"2023-10-28T03:44:17.844869Z","iopub.status.idle":"2023-10-28T03:44:17.853206Z","shell.execute_reply.started":"2023-10-28T03:44:17.844832Z","shell.execute_reply":"2023-10-28T03:44:17.852002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}