{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WLASL Dataset Sign Language Model","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T04:56:14.433145Z","iopub.execute_input":"2023-11-02T04:56:14.433393Z","iopub.status.idle":"2023-11-02T04:56:14.443268Z","shell.execute_reply.started":"2023-11-02T04:56:14.433370Z","shell.execute_reply":"2023-11-02T04:56:14.442378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the JSON file\nwith open('/kaggle/input/wlasl2000-resized/wlasl-complete/WLASL_v0.3.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Words to filter by\nwords_to_filter = [\"where\",\"hello\",\"thank you\", \"go\",\"stop\",\"here\",\"traffic\",\"good\",\"bad\",\"today\"]\n\n# Filter the data\nfiltered_data = [item for item in data if item.get(\"gloss\") in words_to_filter]\n\n# Extract the first 5 \"bbox\" elements from each item\nfor item in filtered_data:\n    item[\"instances\"] = item[\"instances\"][:5]\n\n# Save the filtered data with the first 5 \"bbox\" elements to a JSON file\nwith open('filtered_data_with_5_bbox.json', 'w') as output_file:\n    json.dump(filtered_data, output_file, indent=4)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.449394Z","iopub.execute_input":"2023-11-02T04:56:14.449682Z","iopub.status.idle":"2023-11-02T04:56:14.768182Z","shell.execute_reply.started":"2023-11-02T04:56:14.449658Z","shell.execute_reply":"2023-11-02T04:56:14.767044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/filtered_data_with_5_bbox.json', 'r') as f:\n    data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.770461Z","iopub.execute_input":"2023-11-02T04:56:14.771167Z","iopub.status.idle":"2023-11-02T04:56:14.785264Z","shell.execute_reply.started":"2023-11-02T04:56:14.771125Z","shell.execute_reply":"2023-11-02T04:56:14.784224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[0]['instances'])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.786434Z","iopub.execute_input":"2023-11-02T04:56:14.786752Z","iopub.status.idle":"2023-11-02T04:56:14.800600Z","shell.execute_reply.started":"2023-11-02T04:56:14.786726Z","shell.execute_reply":"2023-11-02T04:56:14.799688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gloss_list = [] # done\nvideo_name_list = [] # done\nstart_frame = [] # done\nend_frame = [] # done\nvid_dir = '/kaggle/input/wlasl2000-resized/wlasl-complete/videos'\nprogress = 0\n\nfor word in filtered_data:\n    for i in range(5):\n        vid_name = os.path.join(vid_dir, f'{word[\"instances\"][i][\"video_id\"]}.mp4')\n        start = word['instances'][i]['frame_start']\n        end = word['instances'][i]['frame_end']\n        label = word['gloss']\n\n        video_name_list.append(vid_name)\n        gloss_list.append(label)\n        start_frame.append(start)\n        end_frame.append(end)\n\n        progress += 1\n        print(f'Progress: {progress} / {len(filtered_data)*5}')","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.802659Z","iopub.execute_input":"2023-11-02T04:56:14.802948Z","iopub.status.idle":"2023-11-02T04:56:14.816475Z","shell.execute_reply.started":"2023-11-02T04:56:14.802924Z","shell.execute_reply":"2023-11-02T04:56:14.815521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'number of words = {len(gloss_list)}\\nnumber of videos = {len(video_name_list)}')","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.817755Z","iopub.execute_input":"2023-11-02T04:56:14.818412Z","iopub.status.idle":"2023-11-02T04:56:14.828842Z","shell.execute_reply.started":"2023-11-02T04:56:14.818376Z","shell.execute_reply":"2023-11-02T04:56:14.827901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip -q install mediapipe\nimport mediapipe as mp\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:14.829978Z","iopub.execute_input":"2023-11-02T04:56:14.830324Z","iopub.status.idle":"2023-11-02T04:56:46.473139Z","shell.execute_reply.started":"2023-11-02T04:56:14.830247Z","shell.execute_reply":"2023-11-02T04:56:46.472174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mp_pose = mp.solutions.pose\nmp_hands = mp.solutions.hands\nmp_face = mp.solutions.face_mesh\n\ndef detect_landmarks(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n         mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands, \\\n         mp_face.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n        \n        pose_results = pose.process(frame_rgb)\n        hand_results = hands.process(frame_rgb)\n        face_results = face_mesh.process(frame_rgb)\n\n    return pose_results, hand_results, face_results","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.474335Z","iopub.execute_input":"2023-11-02T04:56:46.474819Z","iopub.status.idle":"2023-11-02T04:56:46.482004Z","shell.execute_reply.started":"2023-11-02T04:56:46.474792Z","shell.execute_reply":"2023-11-02T04:56:46.481126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_landmarks_from_video(video_path, start_frame=1, end_frame=-1):\n    cap = cv2.VideoCapture(video_path)\n    if end_frame < 0:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if start_frame < 1:\n        start_frame = 1\n    \n    landmarks_data = []\n    \n    with mp_pose.Pose() as pose, mp_hands.Hands() as hands, mp_face.FaceMesh() as face_mesh:\n        while cap.isOpened() and start_frame <= end_frame:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = detect_landmarks(frame)\n            landmarks_data.append({\n                \"frame_number\": start_frame,\n                \"body_pose_landmarks\": results[0].pose_landmarks,\n                \"hand_landmarks\": results[1].multi_hand_landmarks,\n                \"face_mesh_landmarks\": results[2].multi_face_landmarks\n            })\n            start_frame += 1\n\n    cap.release()\n    return landmarks_data","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.482926Z","iopub.execute_input":"2023-11-02T04:56:46.483235Z","iopub.status.idle":"2023-11-02T04:56:46.502744Z","shell.execute_reply.started":"2023-11-02T04:56:46.483202Z","shell.execute_reply":"2023-11-02T04:56:46.502062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_landmarks_on_video(video_path, output_path, start_frame, end_frame, landmarks_data):\n    \n    cap = cv2.VideoCapture(video_path)\n    if end_frame < 0:\n        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if start_frame < 1:\n        start_frame = 1\n\n    frame_count = 1\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened() and frame_count <= end_frame:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count >= start_frame:\n            landmarks = landmarks_data[frame_count - start_frame]\n            frame_with_landmarks = draw_landmarks(frame, landmarks)\n            out.write(frame_with_landmarks)\n            \n        frame_count += 1\n\n    cap.release()\n    out.release()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.503711Z","iopub.execute_input":"2023-11-02T04:56:46.504006Z","iopub.status.idle":"2023-11-02T04:56:46.514629Z","shell.execute_reply.started":"2023-11-02T04:56:46.503976Z","shell.execute_reply":"2023-11-02T04:56:46.513730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_landmarks(frame, landmarks):\n    frame_with_landmarks = frame.copy()\n\n    if landmarks[\"body_pose_landmarks\"]:\n        for landmark in landmarks[\"body_pose_landmarks\"].landmark:\n            x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n            cv2.circle(frame_with_landmarks, (x, y), 2, (0, 0, 255), -1)\n\n    if landmarks[\"hand_landmarks\"]:\n        for hand_landmark in landmarks[\"hand_landmarks\"]:\n            for landmark in hand_landmark.landmark:\n                x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n                cv2.circle(frame_with_landmarks, (x, y), 2, (0, 255, 0), -1)\n\n    if landmarks[\"face_mesh_landmarks\"]:\n        for face_landmarks in landmarks[\"face_mesh_landmarks\"]:\n            for landmark in face_landmarks.landmark:\n                x, y = int(landmark.x * frame.shape[1]), int(landmark.y * frame.shape[0])\n                cv2.circle(frame_with_landmarks, (x, y), 2, (0, 0, 255), -1)\n\n    return frame_with_landmarks\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.518026Z","iopub.execute_input":"2023-11-02T04:56:46.518355Z","iopub.status.idle":"2023-11-02T04:56:46.532236Z","shell.execute_reply.started":"2023-11-02T04:56:46.518328Z","shell.execute_reply":"2023-11-02T04:56:46.531565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vid_dir = '/kaggle/working/new_videos'\nif not os.path.exists(vid_dir):\n    os.mkdir(vid_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.533165Z","iopub.execute_input":"2023-11-02T04:56:46.533422Z","iopub.status.idle":"2023-11-02T04:56:46.545751Z","shell.execute_reply.started":"2023-11-02T04:56:46.533369Z","shell.execute_reply":"2023-11-02T04:56:46.544783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numpy_dir = '/kaggle/working/Numpy_files'\nif not os.path.exists(numpy_dir):\n    os.mkdir(numpy_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.546816Z","iopub.execute_input":"2023-11-02T04:56:46.547122Z","iopub.status.idle":"2023-11-02T04:56:46.563902Z","shell.execute_reply.started":"2023-11-02T04:56:46.547099Z","shell.execute_reply":"2023-11-02T04:56:46.563220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ahh = extract_landmarks_from_video('/kaggle/input/wlasl-processed/videos/00426.mp4', 0, -1)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:56:46.564868Z","iopub.execute_input":"2023-11-02T04:56:46.565131Z","iopub.status.idle":"2023-11-02T04:57:09.157869Z","shell.execute_reply.started":"2023-11-02T04:56:46.565108Z","shell.execute_reply":"2023-11-02T04:57:09.156700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = '/kaggle/input/wlasl-processed/videos/00426.mp4'\noutput_path = '/kaggle/working/ahh.mp4'","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:57:09.159240Z","iopub.execute_input":"2023-11-02T04:57:09.159535Z","iopub.status.idle":"2023-11-02T04:57:09.163628Z","shell.execute_reply.started":"2023-11-02T04:57:09.159511Z","shell.execute_reply":"2023-11-02T04:57:09.162680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_landmarks_on_video(input_path,output_path, 0, -1, ahh)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:57:09.164805Z","iopub.execute_input":"2023-11-02T04:57:09.165098Z","iopub.status.idle":"2023-11-02T04:57:09.624973Z","shell.execute_reply.started":"2023-11-02T04:57:09.165074Z","shell.execute_reply":"2023-11-02T04:57:09.624117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(ahh[1]['hand_landmarks']) ","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:57:09.626185Z","iopub.execute_input":"2023-11-02T04:57:09.626489Z","iopub.status.idle":"2023-11-02T04:57:09.632785Z","shell.execute_reply.started":"2023-11-02T04:57:09.626463Z","shell.execute_reply":"2023-11-02T04:57:09.631888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ahh[0]['hand_landmarks'] == None","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:57:09.634363Z","iopub.execute_input":"2023-11-02T04:57:09.634643Z","iopub.status.idle":"2023-11-02T04:57:09.646672Z","shell.execute_reply.started":"2023-11-02T04:57:09.634618Z","shell.execute_reply":"2023-11-02T04:57:09.645903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_data(frame):\n    pose_landmarks = []\n    face_landmarks = []\n    hands_landmarks = []\n    if frame['body_pose_landmarks'] != None:\n        for landmark in frame['body_pose_landmarks'].landmark:\n            pose_landmarks.append([landmark.x, landmark.y, landmark.z])\n    else:\n        pose_landmarks = np.zeros([1,33,3])\n\n    if frame['hand_landmarks'] != None:\n        for landmark in frame['hand_landmarks'][0].landmark:\n            hands_landmarks.append([landmark.x, landmark.y, landmark.z])\n    else:\n        hands_landmarks = np.zeros([1,21,3])\n\n    if frame['face_mesh_landmarks'] != None:\n        for landmark in frame['face_mesh_landmarks'][0].landmark:\n            face_landmarks.append([landmark.x, landmark.y, landmark.z])\n    else:\n        face_landmarks = np.zeros([1,468,3])\n    if isinstance(pose_landmarks, np.ndarray):\n        pose = pose_landmarks\n    else:\n        pose = np.array([pose_landmarks])\n\n    if isinstance(face_landmarks, np.ndarray):\n        face = face_landmarks\n    else:\n        face = np.array([face_landmarks])\n    if isinstance(hands_landmarks, np.ndarray):\n        hands = hands_landmarks\n    else:\n        hands = np.array([hands_landmarks])\n    stacked_landmarks = np.hstack((pose, hands, face))\n    return (stacked_landmarks)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T04:57:09.647796Z","iopub.execute_input":"2023-11-02T04:57:09.648081Z","iopub.status.idle":"2023-11-02T04:57:09.659068Z","shell.execute_reply.started":"2023-11-02T04:57:09.648056Z","shell.execute_reply":"2023-11-02T04:57:09.658179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_video(landmarks,frame_count):\n    stacked_arrays = [extract_data(landmarks[i]) for i in range(frame_count)]\n    temp = np.vstack(stacked_arrays)\n    return temp","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:02:07.731769Z","iopub.execute_input":"2023-11-02T05:02:07.732622Z","iopub.status.idle":"2023-11-02T05:02:07.737398Z","shell.execute_reply.started":"2023-11-02T05:02:07.732589Z","shell.execute_reply":"2023-11-02T05:02:07.736373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50):\n    input_path = video_name_list[i]\n    start = start_frame[i]\n    end = end_frame[i]\n    npy_path = f'/kaggle/working/Numpy_files/{i}.npy'\n    temp = extract_landmarks_from_video(input_path, start_frame[i],end_frame[i])\n    npyArray = process_video(temp,len(temp))\n    np.save(npy_path,npyArray)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:12:31.897000Z","iopub.execute_input":"2023-11-02T05:12:31.898026Z","iopub.status.idle":"2023-11-02T05:23:28.657093Z","shell.execute_reply.started":"2023-11-02T05:12:31.897981Z","shell.execute_reply":"2023-11-02T05:23:28.656017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r numpy.zip /kaggle/working/Numpy_files","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:24:50.357251Z","iopub.execute_input":"2023-11-02T05:24:50.358020Z","iopub.status.idle":"2023-11-02T05:24:55.566767Z","shell.execute_reply.started":"2023-11-02T05:24:50.357990Z","shell.execute_reply":"2023-11-02T05:24:55.565786Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:46:45.265095Z","iopub.execute_input":"2023-11-02T05:46:45.265514Z","iopub.status.idle":"2023-11-02T05:46:45.270732Z","shell.execute_reply.started":"2023-11-02T05:46:45.265483Z","shell.execute_reply":"2023-11-02T05:46:45.269531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npy_inputs = []\nlabels = []\nfor i in range(50):\n    input = np.load(f'/kaggle/working/Numpy_files/{i}.npy')\n    npy_inputs.append(tf.convert_to_tensor(input))\n    labels.append(gloss_list[i])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:46:47.014070Z","iopub.execute_input":"2023-11-02T05:46:47.014441Z","iopub.status.idle":"2023-11-02T05:46:53.405405Z","shell.execute_reply.started":"2023-11-02T05:46:47.014412Z","shell.execute_reply":"2023-11-02T05:46:53.404322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sequence = 0\nfor i in range(50):\n    f,_,_ = npy_inputs[i].shape\n    max_sequence = max(max_sequence,f)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:55:05.909130Z","iopub.execute_input":"2023-11-02T05:55:05.910043Z","iopub.status.idle":"2023-11-02T05:55:05.914825Z","shell.execute_reply.started":"2023-11-02T05:55:05.910009Z","shell.execute_reply":"2023-11-02T05:55:05.913827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sequence","metadata":{"execution":{"iopub.status.busy":"2023-11-02T05:55:10.181541Z","iopub.execute_input":"2023-11-02T05:55:10.181937Z","iopub.status.idle":"2023-11-02T05:55:10.187995Z","shell.execute_reply.started":"2023-11-02T05:55:10.181904Z","shell.execute_reply":"2023-11-02T05:55:10.187046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_inputs = []\nfor i in range(50):\n    pad_length = max_sequence - npy_inputs[i].shape[0]\n    padded_frame = np.pad(npy_inputs[i], ((0, pad_length), (0, 0), (0, 0)), mode='constant', constant_values=10)\n    new_inputs.append(padded_frame)\n\nnew_inputs = np.array(new_inputs)\nreshaped_inputs = new_inputs[:].reshape(-1, max_sequence, 522 * 3)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:04:14.427978Z","iopub.execute_input":"2023-11-02T06:04:14.428344Z","iopub.status.idle":"2023-11-02T06:04:14.485091Z","shell.execute_reply.started":"2023-11-02T06:04:14.428317Z","shell.execute_reply":"2023-11-02T06:04:14.483908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:10:20.217020Z","iopub.execute_input":"2023-11-02T06:10:20.217688Z","iopub.status.idle":"2023-11-02T06:10:20.240062Z","shell.execute_reply.started":"2023-11-02T06:10:20.217652Z","shell.execute_reply":"2023-11-02T06:10:20.238888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = reshaped_inputs\ny = label_encoder.transform(labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:14:03.821816Z","iopub.execute_input":"2023-11-02T06:14:03.822215Z","iopub.status.idle":"2023-11-02T06:14:03.827196Z","shell.execute_reply.started":"2023-11-02T06:14:03.822181Z","shell.execute_reply":"2023-11-02T06:14:03.826106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:14:04.686345Z","iopub.execute_input":"2023-11-02T06:14:04.687247Z","iopub.status.idle":"2023-11-02T06:14:04.691206Z","shell.execute_reply.started":"2023-11-02T06:14:04.687210Z","shell.execute_reply":"2023-11-02T06:14:04.690267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(X, test_size=0.2, random_state=42)\ntrain_labels, test_labels = train_test_split(y, test_size=0.2, random_state=42)\ntrain_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n\n# Define your neural network model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(max_sequence, 522*3)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:15:47.952771Z","iopub.execute_input":"2023-11-02T06:15:47.953730Z","iopub.status.idle":"2023-11-02T06:15:48.040469Z","shell.execute_reply.started":"2023-11-02T06:15:47.953693Z","shell.execute_reply":"2023-11-02T06:15:48.039681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:15:52.764528Z","iopub.execute_input":"2023-11-02T06:15:52.764925Z","iopub.status.idle":"2023-11-02T06:15:52.776332Z","shell.execute_reply.started":"2023-11-02T06:15:52.764891Z","shell.execute_reply":"2023-11-02T06:15:52.775501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=25)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:24:34.955776Z","iopub.execute_input":"2023-11-02T06:24:34.956180Z","iopub.status.idle":"2023-11-02T06:24:36.712017Z","shell.execute_reply.started":"2023-11-02T06:24:34.956148Z","shell.execute_reply":"2023-11-02T06:24:36.710986Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_data, test_labels)\nprint(f\"Test Accuracy: {test_accuracy*100}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:24:43.714617Z","iopub.execute_input":"2023-11-02T06:24:43.715576Z","iopub.status.idle":"2023-11-02T06:24:43.807339Z","shell.execute_reply.started":"2023-11-02T06:24:43.715538Z","shell.execute_reply":"2023-11-02T06:24:43.806381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T06:24:21.093550Z","iopub.execute_input":"2023-11-02T06:24:21.094484Z","iopub.status.idle":"2023-11-02T06:24:21.113970Z","shell.execute_reply.started":"2023-11-02T06:24:21.094452Z","shell.execute_reply":"2023-11-02T06:24:21.112902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}