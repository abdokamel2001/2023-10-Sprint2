{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdokamel2001/ASL-Translation-Project/blob/main/2023-10-Sprint2-Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61P_xOHYKo71"
      },
      "source": [
        "# Microsoft Dataset Sign Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z44q37G9BxR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umAGvlW1KpRr"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Tasks/2023-10-Sprint2/MS-ASL\" \"/content\" #Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded/filtered.json\" \"/content\" #Pre-Filtered"
      ],
      "metadata": {
        "id": "1MGK8ZYJvF-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlfaKepJWAKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d2680b-0448-4238-c99a-0d5765610507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfcW86rkDHf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from pytube import YouTube\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhPypBlT_Zgw"
      },
      "source": [
        "# Pick 50 Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6usK90M_Z1A"
      },
      "outputs": [],
      "source": [
        "with open('/content/MS-ASL/MSASL_train.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "selected_words = [\"where\", \"hello\", \"thanks\", \"go\", \"stop\", \"here\", \"traffic\", \"good\", \"bad\", \"today\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeO83_0TP153"
      },
      "outputs": [],
      "source": [
        "def check_link(url):\n",
        "    try:\n",
        "        yt = YouTube(url)\n",
        "        yt.check_availability()\n",
        "        if yt.age_restricted:   # doesn't work all the time\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zGzQH9drI1x"
      },
      "outputs": [],
      "source": [
        "word_counts = {word: 0 for word in selected_words}\n",
        "filtered_data = []\n",
        "\n",
        "for dics in data:\n",
        "    word = dics[\"text\"]\n",
        "    url = dics[\"url\"].split('&')[0]\n",
        "\n",
        "    if word in selected_words and check_link(url):\n",
        "        if word_counts[word] < 5:\n",
        "            filtered_data.append(dics)\n",
        "            word_counts[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lorFCNTtrNL9",
        "outputId": "e02ebd0d-7274-4fc1-9b4c-1623256764b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'where': 5,\n",
              " 'hello': 5,\n",
              " 'thanks': 5,\n",
              " 'go': 5,\n",
              " 'stop': 5,\n",
              " 'here': 5,\n",
              " 'traffic': 5,\n",
              " 'good': 5,\n",
              " 'bad': 5,\n",
              " 'today': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crC3_NssrPEt"
      },
      "outputs": [],
      "source": [
        "with open('/content/filtered.json', 'w') as json_file:\n",
        "    json.dump(filtered_data, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZidz-SQUZKv"
      },
      "source": [
        "#MediaPipe Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_T58ZVsURq2"
      },
      "outputs": [],
      "source": [
        "hands = mp.solutions.hands.Hands()\n",
        "pose = mp.solutions.pose.Pose()\n",
        "face_mesh = mp.solutions.face_mesh.FaceMesh()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYas6kbVUUQX"
      },
      "outputs": [],
      "source": [
        "def get_frame_landmarks(frame):\n",
        "    \"\"\"\n",
        "    Extracts landmarks from a single video frame using MediaPipe.\n",
        "\n",
        "    Args:\n",
        "        frame: A single rgb frame/image.\n",
        "\n",
        "    Returns:\n",
        "        np.array: A NumPy array containing extracted landmarks.\n",
        "        The output dimensions are (n, 3) array, where n is the number of landmarks.\n",
        "        Each row in the array represents a landmark, and each landmark is represented\n",
        "        as [x, y, z], where x, y, and z are the normalized coordinates of the landmark.\n",
        "    \"\"\"\n",
        "\n",
        "    results_hands = hands.process(frame)\n",
        "    results_pose = pose.process(frame)\n",
        "    results_face = face_mesh.process(frame)\n",
        "\n",
        "    landmarks_per_hand = 21\n",
        "    landmarks_body_pose = 33\n",
        "    landmarks_face = 468         # Max 468\n",
        "\n",
        "    all_landmarks = np.zeros((landmarks_per_hand * 2 + landmarks_body_pose + landmarks_face, 3))\n",
        "\n",
        "    if results_hands.multi_hand_landmarks:\n",
        "        all_landmarks[:landmarks_per_hand, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[0].landmark])\n",
        "        if len(results_hands.multi_hand_landmarks) > 1:\n",
        "            all_landmarks[landmarks_per_hand:landmarks_per_hand * 2, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_hands.multi_hand_landmarks[1].landmark])\n",
        "\n",
        "    if results_pose.pose_landmarks:\n",
        "        all_landmarks[landmarks_per_hand * 2:landmarks_per_hand * 2 + landmarks_body_pose, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])\n",
        "\n",
        "    if results_face.multi_face_landmarks:\n",
        "        # all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark[::468 // landmarks_face]])\n",
        "        all_landmarks[landmarks_per_hand * 2 + landmarks_body_pose:, :] = np.array([(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])\n",
        "\n",
        "    return all_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPgwih22UWKe"
      },
      "outputs": [],
      "source": [
        "def get_video_landmarks(video_path, start_frame=0, end_frame=-1, num_landmarks=543):\n",
        "    \"\"\"\n",
        "    Extracts landmarks from a video by processing each frame in the video.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The file path to the video to process.\n",
        "        start_frame (int): The index of the starting frame (default is 0).\n",
        "        end_frame (int): The index of the ending frame (default is -1, meaning the last frame).\n",
        "\n",
        "    Returns:\n",
        "        np.array: A NumPy array where each row corresponds to the landmarks\n",
        "        extracted from a single frame of the video within the specified frame range.\n",
        "        The dimensions of the output array are (m, n, 3), where m is the number of frames\n",
        "        within the specified range and n is the number of landmarks.\n",
        "        Each element in the array is a 3D coordinate representing a landmark's position.\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if end_frame < 0:\n",
        "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if start_frame < 0:\n",
        "        start_frame = 0\n",
        "\n",
        "    all_frame_landmarks = np.zeros((end_frame - start_frame, num_landmarks, 3))\n",
        "    frame_index = 0\n",
        "\n",
        "    while cap.isOpened() and frame_index != end_frame:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_index >= start_frame:\n",
        "            frame.flags.writeable = False\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_landmarks = get_frame_landmarks(frame)\n",
        "            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return all_frame_landmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4plkFCSY_Wz"
      },
      "source": [
        "# Load the json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGgzKAttY8Q1"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/filtered.json\", 'r') as json_file:\n",
        "    train_data = json.load(json_file)  # A list of dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrH_FlB6ZAyW"
      },
      "source": [
        "# Define directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrkmuW2QZIVx"
      },
      "outputs": [],
      "source": [
        "video_dir = '/content/train-videos'\n",
        "npy_dir = '/content/train-numpy'\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(npy_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQCzLg7vZnm6"
      },
      "source": [
        "# Iterate through the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "downloaded_video_ids = set()"
      ],
      "metadata": {
        "id": "34KbNPChVZCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GC9AV4Tful7",
        "outputId": "9437a6f3-d6f9-4166-fe5a-45b79228facd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████████████████| 50/50 [16:31<00:00, 19.83s/it]\n"
          ]
        }
      ],
      "source": [
        "failed_video_ids = set()\n",
        "try:\n",
        "    for i in tqdm(range(len(train_data)), ncols=100):\n",
        "        url = train_data[i]['url'].split('&')[0]\n",
        "        video_id = url.split('=')[1]\n",
        "        start = train_data[i][\"start\"]\n",
        "        end = train_data[i][\"end\"]\n",
        "        video_path = os.path.join(video_dir, f'{video_id}.mp4')\n",
        "\n",
        "        label_dir = os.path.join(npy_dir, train_data[i][\"text\"])\n",
        "        os.makedirs(label_dir, exist_ok=True)\n",
        "        npy_path = os.path.join(label_dir, f'{i}.npy')\n",
        "        if os.path.exists(npy_path): continue\n",
        "\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            # stream = yt.streams.get_highest_resolution()\n",
        "            stream = yt.streams.filter(file_extension='mp4').first()\n",
        "            stream.download(output_path=video_dir, filename=f'{video_id}.mp4')\n",
        "            video_landmarks = get_video_landmarks(video_path, start, end)\n",
        "            np.save(npy_path, video_landmarks)\n",
        "            os.remove(video_path)\n",
        "            downloaded_video_ids.add(video_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError downloading {video_id}: {e}\")\n",
        "            if os.path.exists(video_path):\n",
        "                os.remove(video_path)\n",
        "            failed_video_ids.add(video_id)\n",
        "            continue\n",
        "\n",
        "        clear_output(True)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    for f in os.listdir(video_dir):\n",
        "        file_path = os.path.join(video_dir, f)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "    print(\"\\nLoading process interrupted by user.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQNvth68m6Ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edc811e-7800-454b-c76a-c11387bcdcd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Videos (38): {'QUF1JHzBXhw', '2nXrJ_7NOgE', '6XcxbPfP5YQ', 'TwkGS9TjUX8', 'xKAgM2pSEDU', 'PygPDLrGBwE', '75BkNdtDsoQ', 'mC0lNJ6iz-s', 'f7COHRpmVKA', 'WprUBqi3iBc', 'Jwjs1LVxnmE', 'QB44Vddoi-w', '7XHOmZYiBew', 'K8c-np9zNT8', 'i6a81VVo-BM', '3zoqSvF0Z2A', '-LB4ENHxcIs', 'SC9lyDxbwUE', 'u0XAd3TkGaA', 'FVjpLa8GqeM', 'FCPZYdfdabA', 'z8e_-viWx9E', 'CSj7IScvZnE', 'wkxCnCMo7Mc', 'A5tZKVJ195U', 'hjS0dQDgbjo', 'P4QA138QqZc', 'BUhCGlNOqRA', '__lLQ3mhCvM', 'bq-HmgjGzmw', 'DOZJOFHs75s', 'rnr_aY0X0dQ', '7iuyJ84wvds', 'OmpKZvqoyjs', 'p36hZJQpLoQ', 'XtkDeYBnR8o', 'A84uvLUmCVU', 'k0T-yY_HrEQ'}\n",
            "Failed Videos (0): set()\n"
          ]
        }
      ],
      "source": [
        "print(f'Downloaded Videos ({len(downloaded_video_ids)}): {downloaded_video_ids}')\n",
        "print(f'Failed Videos ({len(failed_video_ids)}): {failed_video_ids}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Z4rA82mNHj2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV Function"
      ],
      "metadata": {
        "id": "sfVsq9rUHlBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(input_path, output_path, npy_file, start_frame=0, end_frame=-1):\n",
        "    \"\"\"\n",
        "    Reads a video from the input file, overlays landmarks on each frame, and saves the result to an output video file.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The path to the input video file.\n",
        "        output_path (str): The path to save the output video with overlaid facial landmarks.\n",
        "        npy_file (str): The path to a NumPy file containing facial landmarks data for each frame.\n",
        "        start_frame (int): The index of the starting frame for landmark overlay (default is 0).\n",
        "        end_frame (int): The index of the ending frame for landmark overlay (default is -1, meaning the last frame).\n",
        "\n",
        "    Description:\n",
        "        This function reads a video from the input file, extracts facial landmarks data from a NumPy file,\n",
        "        and overlays landmarks on each frame of the video. The frames within the specified range,\n",
        "        from 'start_frame' (inclusive) to 'end_frame' (exclusive), are processed. Facial landmarks are drawn as\n",
        "        red circles on the face, hands, and body in each frame. The output video is saved to the 'output_path'\n",
        "        with the same resolution and frame rate as the input video.\n",
        "    \"\"\"\n",
        "\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    # fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    landmarks_data = np.load(npy_file)\n",
        "    frame_index = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_index >= start_frame and frame_index < end_frame:\n",
        "            landmarks_frame = landmarks_data[frame_index - start_frame]\n",
        "            landmarks = [(int(x * width), int(y * height)) for x, y, z in landmarks_frame]\n",
        "            for x, y in landmarks:\n",
        "                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
        "\n",
        "        out.write(frame)\n",
        "        frame_index += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()"
      ],
      "metadata": {
        "id": "HHzs6309HccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1]"
      ],
      "metadata": {
        "id": "7N7dqEu9HzRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cb30c8-b716-4a87-f0f0-ccbcb323f3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'org_text': 'stop ',\n",
              " 'clean_text': 'stop',\n",
              " 'start_time': 4.675,\n",
              " 'signer_id': 144,\n",
              " 'signer': -1,\n",
              " 'start': 140,\n",
              " 'end': 220,\n",
              " 'file': 'stop - ASL sign for stop',\n",
              " 'label': 358,\n",
              " 'height': 360.0,\n",
              " 'fps': 29.944,\n",
              " 'end_time': 7.347,\n",
              " 'url': 'https://www.youtube.com/watch?v=Jwjs1LVxnmE',\n",
              " 'review': 1,\n",
              " 'text': 'stop',\n",
              " 'box': [0.0, 0.0, 1.0, 0.9790104627609253],\n",
              " 'width': 480.0}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yt = YouTube(train_data[1][\"url\"].split('&')[0])\n",
        "stream = yt.streams.get_highest_resolution()\n",
        "stream.download(output_path='/content',filename='Original.mp4')"
      ],
      "metadata": {
        "id": "8OfC73R7Mz_9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "483decb4-4891-4db5-90ee-d808a81da21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Original.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_landmarks('/content/Original.mp4', '/content/Edited.mp4', '/content/train-numpy/stop/1.npy', start_frame=140, end_frame=220)"
      ],
      "metadata": {
        "id": "4JrnjzoWHcLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Model Training"
      ],
      "metadata": {
        "id": "ARWhNU0ez0Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded\" \"/content/train-numpy\""
      ],
      "metadata": {
        "id": "3d20_jr10GJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ryqy0ADs0GXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdcgF0Bu1nQJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBAOQjuWfOfg"
      },
      "source": [
        "# **Useful Shortcuts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIJDuF6-1gBF"
      },
      "source": [
        "### Zip to Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrzcDYZ31MgP"
      },
      "outputs": [],
      "source": [
        "!zip -q -r /content/train-numpy.zip -j /content/train-numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2plZs1hb1ig0"
      },
      "source": [
        "### Unzip to Reload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcf_mA3t1cPh"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/train-numpy.zip -d /content/train-numpy/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTs_rGd900wZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5c3cuUerA3"
      },
      "source": [
        "### Export to Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5l0Y7EX5kTa"
      },
      "source": [
        "Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkmhEum_c4zY"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/train-numpy\" \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeMPRlvk5l95"
      },
      "source": [
        ".npz file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs7wcTL44Hs6"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/train-data.npz\" \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViX6eBpJfEvr"
      },
      "source": [
        "### Import from Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAV4OaJC5pBh"
      },
      "source": [
        "Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9WrBvo3eqnn"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded\" \"/content/train-numpy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y52EEV15psg"
      },
      "source": [
        ".npz file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT3hfAH94HtA"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/AI Team/Varying/Microsoft Dataset Encoded/train-data.npz\" \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLgC6yKbfHWX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I25jhFfvzy7l"
      },
      "source": [
        "### Compress .npy to .npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_rGXOHSz3vV"
      },
      "outputs": [],
      "source": [
        "data_dict = {}\n",
        "\n",
        "for filename in os.listdir(npy_dir):\n",
        "    if filename.endswith('.npy'):\n",
        "        key = filename.split('.')[0]\n",
        "        data = np.load(os.path.join(npy_dir, filename), allow_pickle=True)\n",
        "        data_dict[key] = data\n",
        "\n",
        "np.savez_compressed('/content/train-data.npz', **data_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmlxLQo0eYG"
      },
      "source": [
        "### Decompress .npz to .npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G72M_STK00c9"
      },
      "outputs": [],
      "source": [
        "loaded_data = np.load('/content/train-data.npz', allow_pickle=True)\n",
        "\n",
        "for key, data in loaded_data.items():\n",
        "    np.save(os.path.join(npy_dir, f\"{key}.npy\"), data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7UGfIQpc4GV"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}